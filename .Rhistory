# Summarizing edge detection performances
performanceSummary <- function(Omegaks, Omegags, zs, Omega0ks = myData$Omegaks, Omega0gs = myData$Omega0s, z0s = myData$zgks) {
# Calculating Rand indexes and adjusted rand indexes
RI <- randCalc(zs, z0s)
RIadj <- mclust::adjustedRandIndex(zs, z0s)
# Calculating Precision Matrix Error, True Positive Rate, and False Positive Rates
subjSum <- sum(sapply(1:K, FUN = function(k){sum((adj(Omegaks[, , k]) +
adj(Omega0ks[, , k]))[lower.tri(Omega0ks[, , k], diag = FALSE)] == 2)}))
posskEdges <- sum(sapply(1:K, FUN = function(k){sum(adj(Omega0ks[, , k])[lower.tri(Omega0ks[, , k], diag = FALSE)] == 1)}))
TPRk <- subjSum / posskEdges
subjSum0 <- sum(sapply(1:K, FUN = function(k){sum((-1*adj(Omegaks[, , k]) +
adj(Omega0ks[, , k]))[lower.tri(Omega0ks[, , k], diag = FALSE)] == -1)}))
possk0s <- sum(sapply(1:K, FUN = function(k){sum(adj(Omega0ks[, , k])[lower.tri(Omega0ks[, , k], diag = FALSE)] == 0)}))
FPRk <- subjSum0 / possk0s
PrecisionK <- subjSum / (subjSum + subjSum0)
F1k <- 2*(PrecisionK*TPRk) / (PrecisionK + TPRk)
if(is.null(Omegags) == FALSE) {
grpSum <- sum(sapply(1:G, FUN = function(g){sum((adj(Omegags[, , g]) +
adj(Omega0gs[, , g]))[lower.tri(Omega0gs[, , g], diag = FALSE)] == 2)}))
possgEdges <- sum(sapply(1:G, FUN = function(g){sum(adj(Omega0gs[, , g])[lower.tri(Omega0gs[, , g], diag = FALSE)] == 1)}))
TPRg <- grpSum / possgEdges
grpSum0 <- sum(sapply(1:G, FUN = function(g){sum((-1*adj(Omegags[, , g]) +
adj(Omega0gs[, , g]))[lower.tri(Omega0gs[, , g], diag = FALSE)] == -1)}))
possg0s <- sum(sapply(1:G, FUN = function(g){sum(adj(Omega0gs[, , g])[lower.tri(Omega0gs[, , g], diag = FALSE)] == 0)}))
PrecisionG <- grpSum / (grpSum + grpSum0)
F1g <- 2*(PrecisionG*TPRg) / (PrecisionG + TPRg)
FPRg <- grpSum0 / possg0s
} else {
TPRg <- NA
FPRg <- NA
PrecisionG <- NA
F1g <- NA
}
return(data.frame(RI = RI, RIadj = RIadj, TPRk = TPRk, FPRk = FPRk,
PrecisionK = PrecisionK, F1k = F1k,
TPRg = TPRg, FPRg = FPRg, PrecisionG = PrecisionG, F1g = F1g))
}
GGLResults <- lapply(X = c("stARS", "CV"), FUN = function(tune) {
lambda1 <- ifelse(tune == "stARS", optWardggl$lambda1[1],
optTuneCV[which(optTuneCV$method == "GGL"), ]$lambda1)
lambda2 <- ifelse(tune == "stARS", optWardggl$lambda2[1],
optTuneCV[which(optTuneCV$method == "GGL"), ]$lambda2)
# Estimating cluster memberships using Ward clustering based on dissimilarity matrix of Frob norm differences
GGLres <- list()
GGLres$weights <- MatClust(gl, G = G)
# Analyzing using GGL within each estimated cluster
GGLres$res <- unlist(lapply(FUN = function(g) {
prec <- JGL::JGL(Y = myData$simDat[which(as.logical(GGLres$weights[g, ]))], penalty = "group",
penalize.diagonal = FALSE, lambda1 = lambda1 / 100,
lambda2 = lambda2 / 50000, return.whole.theta = TRUE)$theta
return(setNames(prec, c(which(as.logical(GGLres$weights[g, ])))))}, X = 1:G), recursive = F)
GGLzHat <- apply(GGLres$weights, MARGIN = 1, FUN = which.max)
# Estimated group-level network for GGL. Edge present if >= 50% of subjects have it in group
GGLres$Omegags <- array(NA, dim = c(p, p, G))
for(g in 1:G) {
GGLres$Omegags[, , g] <- round(apply(simplify2array(lapply(GGLres$res[which(GGLzHat == g)], FUN = adj)),
MARGIN = 1:2, FUN = mean))
}
return(GGLres)})
tune = "stARS"
lambda1 <- ifelse(tune == "stARS", optWardggl$lambda1[1],
optTuneCV[which(optTuneCV$method == "GGL"), ]$lambda1)
lambda2 <- ifelse(tune == "stARS", optWardggl$lambda2[1],
optTuneCV[which(optTuneCV$method == "GGL"), ]$lambda2)
lambda1
lambda2
# Estimating cluster memberships using Ward clustering based on dissimilarity matrix of Frob norm differences
GGLres <- list()
GGLres$weights <- MatClust(gl, G = G)
GGLres$weights
GGLzHat <- apply(GGLres$weights, MARGIN = 1, FUN = which.max)
GGLzHat
GGLzHat <- apply(GGLres$weights, MARGIN = 2, FUN = which.max)
GGLzHat
# Install version of package used for Biostatistics submission
if("rcm" %in% installed.packages() == FALSE) {
devtools::install_github("dilernia/rcm@biostatistics")
}
# Load package
library(rcm)
# Note this single simulation takes roughly 1.5 minutes to run
# Simulate data
set.seed(1994)
t1 <- Sys.time()
G <- 2
p <- 10
myData <- rccSim(G = G, clustSize = 30, p = p, n = 177, overlap = 0.80, rho = 0.10)
# Standardizing data
myData$simDat <- lapply(myData$simDat, FUN = scale)
# Grid of tuning parameters to search over
lambdas <- expand.grid(lambda1 = c(1, 5, 15, 25, 35, 45), lambda2 = c(5000), lambda3 = 100)
# Find optimal tuning parameter set using modified stARS with 10 bootstrap samples
optTune <- starsRccm(datf = myData$simDat, lambs = lambdas, G = G, N = 10, method = "RCCM")
optWardggl <- starsRccm(datf = myData$simDat, lambs = lambdas, G = G, N = 10, method = "GGL")
optGL <- starsRccm(datf = myData$simDat, lambs = lambdas, G = G, N = 10, method = "GLasso")
# Find optimal tuning parameter set using 5-fold CV
optTuneCV <- cvTune(x = myData$simDat, G = G, lambs = lambdas,
methods = c("RCCM", "GGL", "GLasso"), folds = 5)
# Analyze with optimally selected tuning parameters for RCCM
rccmResults <- lapply(X = c("stARS", "CV"), FUN = function(tune) {
if(tune == "stARS") {
lambda1 <- optTune$lambda1[1]
lambda2 <- optTune$lambda2[1]
lambda3 <- optTune$lambda3[1]
} else {
lambda1 <- optTuneCV[which(optTuneCV$method == "RCCM"), ]$lambda1
lambda2 <- optTuneCV[which(optTuneCV$method == "RCCM"), ]$lambda2
lambda3 <- optTuneCV[which(optTuneCV$method == "RCCM"), ]$lambda3
}
return(rccm(x = myData$simDat, lambda1 = lambda1, lambda2 = lambda2, lambda3 =lambda3, nclusts = G))})
# Using Ward & GGL approach
# Function for calculating pair-wise Frob norm differences and then clustering
MatClust <- function(mats, G) {
K <- dim(mats)[3]
combos <- expand.grid(s1 = 1:K, s2 = 1:K)
distMat <- matrix(NA, nrow = K, ncol = K)
for(r in 1:K) {
for(s in 1:K) {
distMat[r, s] <- norm(mats[, , r] - mats[, , s], type = 'F')
}
}
cl0 <- cutree(hclust(d = as.dist(distMat), method = "ward.D"), k = G)
wgk <- matrix(NA, nrow = G, ncol = K)
for(i in 1:G) {
for(j in 1:K) {
wgk[i, j] <- ifelse(cl0[j] == i, 1, 0)
}
}
return(wgk)
}
K <- length(myData$simDat)
Sl <- sapply(myData$simDat, cov, simplify = "array")
gl <- sapply(1:K, simplify = "array", FUN = function(x){glasso::glasso(Sl[, , x], rho = 0.001,
penalize.diagonal = FALSE)$wi})
GGLResults <- lapply(X = c("stARS", "CV"), FUN = function(tune) {
lambda1 <- ifelse(tune == "stARS", optWardggl$lambda1[1],
optTuneCV[which(optTuneCV$method == "GGL"), ]$lambda1)
lambda2 <- ifelse(tune == "stARS", optWardggl$lambda2[1],
optTuneCV[which(optTuneCV$method == "GGL"), ]$lambda2)
# Estimating cluster memberships using Ward clustering based on dissimilarity matrix of Frob norm differences
GGLres <- list()
GGLres$weights <- MatClust(gl, G = G)
# Analyzing using GGL within each estimated cluster
GGLres$res <- unlist(lapply(FUN = function(g) {
prec <- JGL::JGL(Y = myData$simDat[which(as.logical(GGLres$weights[g, ]))], penalty = "group",
penalize.diagonal = FALSE, lambda1 = lambda1 / 100,
lambda2 = lambda2 / 50000, return.whole.theta = TRUE)$theta
return(setNames(prec, c(which(as.logical(GGLres$weights[g, ])))))}, X = 1:G), recursive = F)
GGLzHat <- apply(GGLres$weights, MARGIN = 2, FUN = which.max)
# Estimated group-level network for GGL. Edge present if >= 50% of subjects have it in group
GGLres$Omegags <- array(NA, dim = c(p, p, G))
for(g in 1:G) {
GGLres$Omegags[, , g] <- round(apply(simplify2array(lapply(GGLres$res[which(GGLzHat == g)], FUN = adj)),
MARGIN = 1:2, FUN = mean))
}
return(GGLres)})
# Using GLasso & K-means clustering
GLassoResults <- lapply(X = c("stARS", "CV"), FUN = function(tune) {
GLassores <- list()
lambda1 <- ifelse(tune == "stARS", optGL$lambda1[1],
optTuneCV[which(optTuneCV$method == "RCCM"), ]$lambda1)
GLassores$res <- lapply(myData$simDat, FUN = function(x) {
glasso::glasso(cov(x), rho = lambda1 / 100, penalize.diagonal = FALSE)$wi})
# Creating matrix of vectorized precision matrix estimates
vMat <- do.call(rbind, lapply(X = GLassores$res, FUN = as.numeric))
# Finding estimated cluster memberships using k-means clustering
GLassores$weights <- as.integer(factor(kmeans(x = vMat, centers = G)$cluster, levels = c(1:G)))
return(list(GLassores))})
# Summarizing edge detection performances
performanceSummary <- function(Omegaks, Omegags, zs, Omega0ks = myData$Omegaks, Omega0gs = myData$Omega0s, z0s = myData$zgks) {
# Calculating Rand indexes and adjusted rand indexes
RI <- randCalc(zs, z0s)
RIadj <- mclust::adjustedRandIndex(zs, z0s)
# Calculating Precision Matrix Error, True Positive Rate, and False Positive Rates
subjSum <- sum(sapply(1:K, FUN = function(k){sum((adj(Omegaks[, , k]) +
adj(Omega0ks[, , k]))[lower.tri(Omega0ks[, , k], diag = FALSE)] == 2)}))
posskEdges <- sum(sapply(1:K, FUN = function(k){sum(adj(Omega0ks[, , k])[lower.tri(Omega0ks[, , k], diag = FALSE)] == 1)}))
TPRk <- subjSum / posskEdges
subjSum0 <- sum(sapply(1:K, FUN = function(k){sum((-1*adj(Omegaks[, , k]) +
adj(Omega0ks[, , k]))[lower.tri(Omega0ks[, , k], diag = FALSE)] == -1)}))
possk0s <- sum(sapply(1:K, FUN = function(k){sum(adj(Omega0ks[, , k])[lower.tri(Omega0ks[, , k], diag = FALSE)] == 0)}))
FPRk <- subjSum0 / possk0s
PrecisionK <- subjSum / (subjSum + subjSum0)
F1k <- 2*(PrecisionK*TPRk) / (PrecisionK + TPRk)
if(is.null(Omegags) == FALSE) {
grpSum <- sum(sapply(1:G, FUN = function(g){sum((adj(Omegags[, , g]) +
adj(Omega0gs[, , g]))[lower.tri(Omega0gs[, , g], diag = FALSE)] == 2)}))
possgEdges <- sum(sapply(1:G, FUN = function(g){sum(adj(Omega0gs[, , g])[lower.tri(Omega0gs[, , g], diag = FALSE)] == 1)}))
TPRg <- grpSum / possgEdges
grpSum0 <- sum(sapply(1:G, FUN = function(g){sum((-1*adj(Omegags[, , g]) +
adj(Omega0gs[, , g]))[lower.tri(Omega0gs[, , g], diag = FALSE)] == -1)}))
possg0s <- sum(sapply(1:G, FUN = function(g){sum(adj(Omega0gs[, , g])[lower.tri(Omega0gs[, , g], diag = FALSE)] == 0)}))
PrecisionG <- grpSum / (grpSum + grpSum0)
F1g <- 2*(PrecisionG*TPRg) / (PrecisionG + TPRg)
FPRg <- grpSum0 / possg0s
} else {
TPRg <- NA
FPRg <- NA
PrecisionG <- NA
F1g <- NA
}
return(data.frame(RI = RI, RIadj = RIadj, TPRk = TPRk, FPRk = FPRk,
PrecisionK = PrecisionK, F1k = F1k,
TPRg = TPRg, FPRg = FPRg, PrecisionG = PrecisionG, F1g = F1g))
}
View(rccmResults)
performanceSummary(Omegaks = rccmResults[[1]]$Omegas, Omegags =rccmResults[[1]]$Omega0,
zs = apply(rccmResults[[1]]$weights, FUN = which.max, MARGIN = 2))
performanceSummary(Omegaks = rccmResults[[2]]$Omegas, Omegags =rccmResults[[2]]$Omega0,
zs = apply(rccmResults[[2]]$weights, FUN = which.max, MARGIN = 2))
performanceSummary(Omegaks = GGLResults[[1]]$res, Omegags = GGLResults[[1]]$Omegags,
zs = apply(GGLResults[[1]]$weights, FUN = which.max, MARGIN = 2))
performanceSummary(Omegaks = simplify2array(GGLResults[[1]]$res), Omegags = GGLResults[[1]]$Omegags,
zs = apply(GGLResults[[1]]$weights, FUN = which.max, MARGIN = 2))
performanceSummary(Omegaks = simplify2array(GGLResults[[2]]$res), Omegags = GGLResults[[2]]$Omegags,
zs = apply(GGLResults[[2]]$weights, FUN = which.max, MARGIN = 2))
View(GLassoResults)
performanceSummary(Omegaks = simplify2array(GLassoResults[[1]][[1]]$res), Omegags = NULL, zs = apply(GLassoResults[[1]][[1]]$weights, MARGIN = 2, FUN = which.max))
performanceSummary(Omegaks = simplify2array(GLassoResults[[1]][[1]]$res), Omegags = NULL, zs = apply(GLassoResults[[1]][[1]]$weights, MARGIN = 2, FUN = which.max))
GLassoResults[[1]][[1]]$weights
performanceSummary(Omegaks = simplify2array(GLassoResults[[1]][[1]]$res), Omegags = NULL, zs = GLassoResults[[1]][[1]]$weights)
performanceSummary(Omegaks = simplify2array(GLassoResults[[1]][[1]]$res),
Omegags = NULL, zs = GLassoResults[[1]][[1]]$weights)
performanceSummary(Omegaks = simplify2array(GLassoResults[[2]][[1]]$res),
Omegags = NULL, zs = GLassoResults[[2]][[1]]$weights)
# Install version of package used for Biostatistics submission
if("rcm" %in% installed.packages() == FALSE) {
devtools::install_github("dilernia/rcm@biostatistics")
}
# Load package
library(rcm)
# Note this single simulation takes roughly 1.5 minutes to run
# Simulate data
set.seed(1994)
t1 <- Sys.time()
G <- 2
p <- 10
myData <- rccSim(G = G, clustSize = 30, p = p, n = 177, overlap = 0.20, rho = 0.10)
# Standardizing data
myData$simDat <- lapply(myData$simDat, FUN = scale)
# Grid of tuning parameters to search over
lambdas <- expand.grid(lambda1 = c(1, 5, 15, 25, 35, 45), lambda2 = c(5000), lambda3 = 100)
# Find optimal tuning parameter set using modified stARS with 10 bootstrap samples
optTune <- starsRccm(datf = myData$simDat, lambs = lambdas, G = G, N = 10, method = "RCCM")
optWardggl <- starsRccm(datf = myData$simDat, lambs = lambdas, G = G, N = 10, method = "GGL")
optGL <- starsRccm(datf = myData$simDat, lambs = lambdas, G = G, N = 10, method = "GLasso")
# Find optimal tuning parameter set using 5-fold CV
optTuneCV <- cvTune(x = myData$simDat, G = G, lambs = lambdas,
methods = c("RCCM", "GGL", "GLasso"), folds = 5)
# Analyze with optimally selected tuning parameters for RCCM
rccmResults <- lapply(X = c("stARS", "CV"), FUN = function(tune) {
if(tune == "stARS") {
lambda1 <- optTune$lambda1[1]
lambda2 <- optTune$lambda2[1]
lambda3 <- optTune$lambda3[1]
} else {
lambda1 <- optTuneCV[which(optTuneCV$method == "RCCM"), ]$lambda1
lambda2 <- optTuneCV[which(optTuneCV$method == "RCCM"), ]$lambda2
lambda3 <- optTuneCV[which(optTuneCV$method == "RCCM"), ]$lambda3
}
return(rccm(x = myData$simDat, lambda1 = lambda1, lambda2 = lambda2, lambda3 =lambda3, nclusts = G))})
# Using Ward & GGL approach
# Function for calculating pair-wise Frob norm differences and then clustering
MatClust <- function(mats, G) {
K <- dim(mats)[3]
combos <- expand.grid(s1 = 1:K, s2 = 1:K)
distMat <- matrix(NA, nrow = K, ncol = K)
for(r in 1:K) {
for(s in 1:K) {
distMat[r, s] <- norm(mats[, , r] - mats[, , s], type = 'F')
}
}
cl0 <- cutree(hclust(d = as.dist(distMat), method = "ward.D"), k = G)
wgk <- matrix(NA, nrow = G, ncol = K)
for(i in 1:G) {
for(j in 1:K) {
wgk[i, j] <- ifelse(cl0[j] == i, 1, 0)
}
}
return(wgk)
}
K <- length(myData$simDat)
Sl <- sapply(myData$simDat, cov, simplify = "array")
gl <- sapply(1:K, simplify = "array", FUN = function(x){glasso::glasso(Sl[, , x], rho = 0.001,
penalize.diagonal = FALSE)$wi})
GGLResults <- lapply(X = c("stARS", "CV"), FUN = function(tune) {
lambda1 <- ifelse(tune == "stARS", optWardggl$lambda1[1],
optTuneCV[which(optTuneCV$method == "GGL"), ]$lambda1)
lambda2 <- ifelse(tune == "stARS", optWardggl$lambda2[1],
optTuneCV[which(optTuneCV$method == "GGL"), ]$lambda2)
# Estimating cluster memberships using Ward clustering based on dissimilarity matrix of Frob norm differences
GGLres <- list()
GGLres$weights <- MatClust(gl, G = G)
# Analyzing using GGL within each estimated cluster
GGLres$res <- unlist(lapply(FUN = function(g) {
prec <- JGL::JGL(Y = myData$simDat[which(as.logical(GGLres$weights[g, ]))], penalty = "group",
penalize.diagonal = FALSE, lambda1 = lambda1 / 100,
lambda2 = lambda2 / 50000, return.whole.theta = TRUE)$theta
return(setNames(prec, c(which(as.logical(GGLres$weights[g, ])))))}, X = 1:G), recursive = F)
GGLzHat <- apply(GGLres$weights, MARGIN = 2, FUN = which.max)
# Estimated group-level network for GGL. Edge present if >= 50% of subjects have it in group
GGLres$Omegags <- array(NA, dim = c(p, p, G))
for(g in 1:G) {
GGLres$Omegags[, , g] <- round(apply(simplify2array(lapply(GGLres$res[which(GGLzHat == g)], FUN = adj)),
MARGIN = 1:2, FUN = mean))
}
return(GGLres)})
# Using GLasso & K-means clustering
GLassoResults <- lapply(X = c("stARS", "CV"), FUN = function(tune) {
GLassores <- list()
lambda1 <- ifelse(tune == "stARS", optGL$lambda1[1],
optTuneCV[which(optTuneCV$method == "RCCM"), ]$lambda1)
GLassores$res <- lapply(myData$simDat, FUN = function(x) {
glasso::glasso(cov(x), rho = lambda1 / 100, penalize.diagonal = FALSE)$wi})
# Creating matrix of vectorized precision matrix estimates
vMat <- do.call(rbind, lapply(X = GLassores$res, FUN = as.numeric))
# Finding estimated cluster memberships using k-means clustering
GLassores$weights <- as.integer(factor(kmeans(x = vMat, centers = G)$cluster, levels = c(1:G)))
return(list(GLassores))})
# Summarizing edge detection performances
performanceSummary <- function(Omegaks, Omegags, zs, Omega0ks = myData$Omegaks, Omega0gs = myData$Omega0s, z0s = myData$zgks) {
# Calculating Rand indexes and adjusted rand indexes
RI <- randCalc(zs, z0s)
RIadj <- mclust::adjustedRandIndex(zs, z0s)
# Calculating Precision Matrix Error, True Positive Rate, and False Positive Rates
subjSum <- sum(sapply(1:K, FUN = function(k){sum((adj(Omegaks[, , k]) +
adj(Omega0ks[, , k]))[lower.tri(Omega0ks[, , k], diag = FALSE)] == 2)}))
posskEdges <- sum(sapply(1:K, FUN = function(k){sum(adj(Omega0ks[, , k])[lower.tri(Omega0ks[, , k], diag = FALSE)] == 1)}))
TPRk <- subjSum / posskEdges
subjSum0 <- sum(sapply(1:K, FUN = function(k){sum((-1*adj(Omegaks[, , k]) +
adj(Omega0ks[, , k]))[lower.tri(Omega0ks[, , k], diag = FALSE)] == -1)}))
possk0s <- sum(sapply(1:K, FUN = function(k){sum(adj(Omega0ks[, , k])[lower.tri(Omega0ks[, , k], diag = FALSE)] == 0)}))
FPRk <- subjSum0 / possk0s
PrecisionK <- subjSum / (subjSum + subjSum0)
F1k <- 2*(PrecisionK*TPRk) / (PrecisionK + TPRk)
if(is.null(Omegags) == FALSE) {
grpSum <- sum(sapply(1:G, FUN = function(g){sum((adj(Omegags[, , g]) +
adj(Omega0gs[, , g]))[lower.tri(Omega0gs[, , g], diag = FALSE)] == 2)}))
possgEdges <- sum(sapply(1:G, FUN = function(g){sum(adj(Omega0gs[, , g])[lower.tri(Omega0gs[, , g], diag = FALSE)] == 1)}))
TPRg <- grpSum / possgEdges
grpSum0 <- sum(sapply(1:G, FUN = function(g){sum((-1*adj(Omegags[, , g]) +
adj(Omega0gs[, , g]))[lower.tri(Omega0gs[, , g], diag = FALSE)] == -1)}))
possg0s <- sum(sapply(1:G, FUN = function(g){sum(adj(Omega0gs[, , g])[lower.tri(Omega0gs[, , g], diag = FALSE)] == 0)}))
PrecisionG <- grpSum / (grpSum + grpSum0)
F1g <- 2*(PrecisionG*TPRg) / (PrecisionG + TPRg)
FPRg <- grpSum0 / possg0s
} else {
TPRg <- NA
FPRg <- NA
PrecisionG <- NA
F1g <- NA
}
return(data.frame(RI = RI, RIadj = RIadj, TPRk = TPRk, FPRk = FPRk,
PrecisionK = PrecisionK, F1k = F1k,
TPRg = TPRg, FPRg = FPRg, PrecisionG = PrecisionG, F1g = F1g))
}
# Performances using stARS
performanceSummary(Omegaks = rccmResults[[1]]$Omegas, Omegags =rccmResults[[1]]$Omega0,
zs = apply(rccmResults[[1]]$weights, FUN = which.max, MARGIN = 2))
performanceSummary(Omegaks = simplify2array(GGLResults[[1]]$res), Omegags = GGLResults[[1]]$Omegags,
zs = apply(GGLResults[[1]]$weights, FUN = which.max, MARGIN = 2))
performanceSummary(Omegaks = simplify2array(GLassoResults[[1]][[1]]$res),
Omegags = NULL, zs = GLassoResults[[1]][[1]]$weights)
# Performances using 5-fold CV
performanceSummary(Omegaks = rccmResults[[2]]$Omegas, Omegags =rccmResults[[2]]$Omega0,
zs = apply(rccmResults[[2]]$weights, FUN = which.max, MARGIN = 2))
performanceSummary(Omegaks = simplify2array(GGLResults[[2]]$res), Omegags = GGLResults[[2]]$Omegags,
zs = apply(GGLResults[[2]]$weights, FUN = which.max, MARGIN = 2))
performanceSummary(Omegaks = simplify2array(GLassoResults[[2]][[1]]$res),
Omegags = NULL, zs = GLassoResults[[2]][[1]]$weights)
t2 <- Sys.time()
t2 - t1
?rcm::rccSim
choose(10, 2)
10*9/2
G = 2
clustSize = 60
p = 10
n = 177
overlap = 0.50
rho = 0.10
esd = 0.05
type = "hub"
eprob = 0.50
# Calculating total number of subjects
K <- ifelse(length(clustSize) == 1, G * clustSize, sum(clustSize))
clustSize = 30
# Calculating total number of subjects
K <- ifelse(length(clustSize) == 1, G * clustSize, sum(clustSize))
# Cluster Networks --------------------------------------------------------
g0s <- array(0, c(p, p, G))
gks <- array(0, c(p, p, K))
Omega0s <- array(0, c(p, p, G))
Omegaks <- array(0, c(p, p, K))
if(length(clustSize) != 1 & length(clustSize) != G) {
stop("clustSize must be of length 1 or of length equal to the number of clusters")
}
else {
if (length(clustSize) > 1) {
zgks <- c()
for (g in 1:length(clustSize)) {
zgks <- c(zgks, rep(g, clustSize[g]))
}
} else {
zgks <- sort(rep(1:G, clustSize))
}
}
simDat <- list()
# Cluster Networks --------------------------------------------------------
g0s <- array(0, c(p, p, G))
gks <- array(0, c(p, p, K))
Omega0s <- array(0, c(p, p, G))
Omegaks <- array(0, c(p, p, K))
if(length(clustSize) != 1 & length(clustSize) != G) {
stop("clustSize must be of length 1 or of length equal to the number of clusters")
} else {
if (length(clustSize) > 1) {
zgks <- c()
for (g in 1:length(clustSize)) {
zgks <- c(zgks, rep(g, clustSize[g]))
}
} else {
zgks <- sort(rep(1:G, clustSize))
}
}
simDat <- list()
zgks
# Number of hubs
J <- floor(sqrt(p))
# Function for dividing entries by corresponding row sums to make positive definite
symmPosDef <- function(m) {
m <- m + t(m)
smallE <- min(eigen(m)$values)
if (smallE <= 0) {
m <- m + diag(rep(abs(smallE) + 0.10 + 0.10, times = nrow(m)))
}
return(m)
}
# Determining edges to be shared across groups
numE <- p - J
q <- choose(p, 2)
numShare <- ifelse(type == "hub", floor(numE * overlap), floor(q * overlap))
eShare <- matrix(which(lower.tri(matrix(1, p, p), diag = F),
arr.ind = TRUE)[sample(1:q, size = numShare), ], ncol = 2)
shared <- sample(c(1, 0), size = nrow(eShare), replace = TRUE, prob = c(eprob, 1 - eprob))
# Different graphs if balanced clusters or not
balanced <- ifelse(length(clustSize) > 1, "_unbal", "")
eShare
shared
# Different graphs if balanced clusters or not
balanced <- ifelse(length(clustSize) > 1, "_unbal", "")
# Generating group-level graphs and precision matrices
while (min(apply(Omega0s, MARGIN = 3, FUN = function(x) {
min(eigen(x)$values)})) <= 0) {
for (g in 1:G) {
g0s[, , g] <- matrix(0, nrow = p, ncol = p)
if(type == "hub") {
hubs <- split(sample.int(p, size = p, replace = FALSE), rep(1:J, ceiling(p / J))[1:p])
for (h in 1:J) {
for (v in hubs[[h]]) {
g0s[, , g][hubs[[h]][1], v] <- 1
}
}
} else if(type == "random") {
offInds <- lower.tri(g0s[, , g], diag = FALSE)
g0s[, , g][offInds] <- sample(c(1, 0), size = sum(offInds),
replace = TRUE, prob = c(eprob, 1 - eprob))
}
# Adding in numShare shared edges
for (e in 1:nrow(eShare)) {
g0s[, , g][eShare[e, 1], eShare[e, 2]] <- shared[e]
}
# Saving graphs to keep constant across simulations
g0s[, , g] <- (g0s[, , g] + t(g0s[, , g]) > 0.001) + 0
# Making graph triangular for precision matrix generation and storing row edge count
g0s[, , g] <- (g0s[, , g] + t(g0s[, , g]) > 0.001) + 0
rwSum <- rowSums(g0s[, , g])
g0s[, , g][upper.tri(g0s[, , g], diag = T)] <- 0
Omega0s[, , g] <- g0s[, , g] * matrix(runif(n = p * p, min = 0.50, max = 1) * sample(c(1, -1),
size = p * p, replace = T), nrow = p, ncol = p)
if (g > 1) {
for (e in 1:nrow(eShare)) {
Omega0s[, , g][eShare[e, 1], eShare[e, 2]] <- Omega0s[, , g - 1][eShare[e, 1], eShare[e, 2]]
}
}
# Making matrix symmetric and positive definite
Omega0s[, , g] <- symmPosDef(Omega0s[, , g])
# Making graph full again, not just lower triangular
g0s[, , g] <- g0s[, , g] + t(g0s[, , g])
}
}
min(apply(Omegaks, MARGIN = 3, FUN = function(x) {
min(eigen(x)$values)})) <= 0
k = 1
# Creating subject-level graph to be exactly same as group-level graph for now
gks[, , k] <- matrix(0, nrow = p, ncol = p)
gks[, , k][lower.tri(gks[, , k], diag = FALSE)] <- g0s[, , zgks[k]][lower.tri(g0s[, , zgks[k]],
diag = FALSE)]
# Forcing subject-level matrix to have similar value as group-level matrix
Omegaks[, , k] <- gks[, , k] * (Omega0s[, , zgks[k]] + matrix(rnorm(n = p * p, sd = esd),
nrow = p, ncol = p))
Omegaks[, , k]
floor(rho * sum(gks[, , k]))
sum(gks[, , k])
sum(gks[, , k])*0.10
round(sum(gks[, , k])*0.10)
round(0.1)
round(0.5)
round(0.51)
devtools::document()
?gapSelect
devtools::document()
